https://docs.google.com/spreadsheets/d/1IxmiL6end-0r8rgzrNJyofkusADsa0XUtYaYXYZRQvM/edit#gid=0


import subprocess

PROJECTS = ["project1-id", "project2-id", "project3-id"]

for project_id in PROJECTS:
    print(f"Project: {project_id}\n")

    # Get a list of Cloud SQL instances
    instances_output = subprocess.check_output(
        f"gcloud sql instances list --project {project_id} --format='value(NAME)'",
        shell=True,
    )
    instances = instances_output.decode("utf-8").strip().split("\n")

    # Iterate over each instance
    for instance in instances:
        # Get the labels for the instance
        labels_output = subprocess.check_output(
            f"gcloud sql instances describe {instance} --project {project_id} --format='value(labels)'",
            shell=True,
        )
        labels = labels_output.decode("utf-8").strip()

        # Check if the label 'terraformed=true' is not present
        if "terraformed=true" not in labels:
            print(instance)

    print()







#!/bin/bash

CONNECTION_NAME="YOUR_CONNECTION_NAME"

# Get a list of connection IDs matching the specified name
connection_ids=$(airflow connections list | awk -v name="$CONNECTION_NAME" '$2 == name {print $1}')

# Loop through the connection IDs and delete each connection
for conn_id in $connection_ids; do
    airflow connections --delete --conn_id "$conn_id"
done


import json
from airflow.hooks.base_hook import BaseHook

connections = BaseHook.get_connections()

json_file = '/path/to/connections.json'

connections_data = []
for connection in connections:
    connections_data.append({
        'conn_id': connection.conn_id,
        'conn_type': connection.conn_type,
        'host': connection.host,
        'login': connection.login,
        'password': connection.password,
        'schema': connection.schema,
        'port': connection.port,
        'extra': connection.extra
    })

with open(json_file, 'w') as file:
    json.dump(connections_data, file, indent=4)

print(f"Connections exported to {json_file}")





import json
from airflow import models

connections = models.Connection().find_all()

json_file = '/path/to/connections.json'

connections_data = []
for connection in connections:
    connections_data.append({
        'conn_id': connection.conn_id,
        'conn_type': connection.conn_type,
        'host': connection.host,
        'login': connection.login,
        'password': connection.password,
        'schema': connection.schema,
        'port': connection.port,
        'extra': connection.extra
    })

with open(json_file, 'w') as file:
    json.dump(connections_data, file, indent=4)

print(f"Connections exported to {json_file}")



#!/bin/bash

PROJECTS=("project1-id" "project2-id" "project3-id")

# Iterate over each project
for PROJECT_ID in "${PROJECTS[@]}"; do
  echo "Project: ${PROJECT_ID}"

  # Get a list of Cloud SQL instances
  INSTANCES=$(gcloud sql instances list --project "${PROJECT_ID}" --format "value(NAME)")

  # Iterate over each instance
  for INSTANCE in ${INSTANCES}; do
    # Get the labels for the instance
    LABELS=$(gcloud sql instances describe "${INSTANCE}" --project "${PROJECT_ID}" --format "value(labels)")

    # Check if the label 'terraformed=true' is not present
    if [[ ! "${LABELS}" =~ terraformed=true ]]; then
      echo "${INSTANCE}"
    fi
  done

  echo
done

